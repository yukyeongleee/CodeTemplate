{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukyeongleee/CodeTemplate/blob/main/M06_tutorial_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xfiwb8iVXRT"
      },
      "source": [
        "# U-Net 기반 표현학습\n",
        "\n",
        "이번 튜토리얼에서는 수정된 U-Net 모델을 정의하고, 복원 손실 함수(Reconstruction loss)로 이를 학습합니다. 또한 학습한 인코더의 뒷단에 간단한 MLP 기반의 분류기를 결합하여 downstream 분류 과제를 해결하는데 사용해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2aHCTHBUMvZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import torchvision.utils as vutils\n",
        "from torchvision.datasets import Caltech101\n",
        "from torchvision import transforms\n",
        "\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage 1을 위한 하이퍼파라미터"
      ],
      "metadata": {
        "id": "EMpAPKxQQ8Re"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afYFkiy1S723"
      },
      "outputs": [],
      "source": [
        "image_size = 128\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "learning_rate = 1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqYkrjGsgZEN"
      },
      "source": [
        "## Tensorboard 로깅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8YmUvpOgbtp"
      },
      "outputs": [],
      "source": [
        "now = datetime.now()\n",
        "formattedDate = now.strftime(\"%H%M%S\")\n",
        "print(formattedDate)\n",
        "\n",
        "run_name = f'unet-{formattedDate}'\n",
        "\n",
        "log_dir = os.path.join('./runs', run_name)\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "writer1 = SummaryWriter(log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbic8jDDhRgh"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "# %reload_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzaXZGgCE07p"
      },
      "source": [
        "## Step 1. Encoder 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eJxqmFeVf-6"
      },
      "source": [
        "### Step 1-1. EncoderBlock 정의\n",
        "\n",
        "인코더(수축경로)의 기본 단위가 될 인코더 블록을 정의합니다.\n",
        "\n",
        "*  인코더 블록은 **두 개의 Conv2d 레이어**와 **마지막 MaxPool2d 레이어**로 구성됩니다. 각 Conv2d의 출력은 **ReLU 활성화함수**를 통과합니다.\n",
        "*  두 개의 Conv2d 레이어 모두 (kernel_size, stride, padding) = (3, 1, 1)로 설정합니다. 따라서 Conv2d 레이어의 출력은 입력과 동일한 모양을 갖습니다.\n",
        "*  인코더 블록은 **두 가지 맵: MaxPool2d 레이어의 출력(`out`)과 입력(`feat`)**을 반환합니다. `feat`은 기록해뒀다가 이후 Skip Connection에 활용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn8z7_e8CAIK"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.layers = None  # Hint. nn.Sequential\n",
        "        self.down = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.layers(x)\n",
        "        out = self.down(feat)\n",
        "        return out, feat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-hJxuqlCyXe"
      },
      "source": [
        "### Step 1-2. Encoder 정의\n",
        "\n",
        "인코더 블록을 조합하여 인코더를 정의합니다.  \n",
        "\n",
        "*  인코더는 네 개의 인코더 블록으로 구성됩니다. 각 블록의 입출력 채널 수는 다음과 같습니다.\n",
        "    * Block 1.  `in_channels` → `model_channels`\n",
        "    * Block 2.  `model_channels` → `model_channels * 2`\n",
        "    * Block 3.  `model_channels * 2` → `model_channels * 4`\n",
        "    * Block 4.  `model_channels * 4` → `model_channels * 8`\n",
        "*  각 블록의 출력 가운데 `out`은 다음 블록으로 전달하고, `feat`은 `features`라는 이름의 리스트에 모아두세요.\n",
        "*  인코더는 최종 출력(`x`)와 중간 특성들을 모아둔 리스트(`features`)를 반환합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBh6LkFTCp_P"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, model_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = None # Hint. nn.ModuleList\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for block in self.blocks:\n",
        "            pass\n",
        "\n",
        "        return (x, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdvvLESxXxhC"
      },
      "source": [
        "## Step 2. DecoderBlock 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej1tbinZE-4K"
      },
      "source": [
        "### Step 2-1. DecoderBlock 정의\n",
        "\n",
        "디코더(확장경로)의 기본 단위가 될 디코더 블록을 정의합니다.\n",
        "\n",
        "*  디코더 블록은 **첫번째 ConvTranspose 레이어**와 **두 개의 Conv2d 레이어**로 구성됩니다. 각 Conv2d의 출력은 **ReLU 활성화함수**를 통과합니다.\n",
        "*  두 개의 Conv2d 레이어 모두 (kernel_size, stride, padding) = (3, 1, 1)로 설정합니다. 따라서 Conv2d 레이어의 출력은 입력과 동일한 모양을 갖습니다.\n",
        "*  디코더 블록은 **두 가지 맵: 이전 블록의 출력(`x`)과 대응하는 인코더 블록의 중간 특성(`encoder_features`)**을 입력받습니다. `x`를 ConvTranspose 레이어에 통과시켜 나온 특성과 `encoder_features`를 채널 방향으로 이어붙이세요. 이어붙인 결과를 이후 두 개의 Conv2d 레이어로 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7mjMjcPHCLC"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = None\n",
        "        self.layers = None # Hint. nn.Sequential\n",
        "\n",
        "    def forward(self, x, encoder_features):\n",
        "\n",
        "        # Hint. torch.cat\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cYCbu6qFGRx"
      },
      "source": [
        "### Step 2-2. Decoder 정의\n",
        "\n",
        "디코더 블록을 조합하여 디코더를 정의합니다.  \n",
        "\n",
        "*  디코더는 네 개의 디코더 블록으로 구성됩니다. 각 블록의 입출력 채널 수는 다음과 같습니다.\n",
        "    * Block 1.  `in_channels` → `model_channels`\n",
        "    * Block 2.  `model_channels` → `model_channels * 2`\n",
        "    * Block 3.  `model_channels * 2` → `model_channels * 4`\n",
        "    * Block 4.  `model_channels * 4` → `model_channels * 8`\n",
        "*  전달받은 인코더의 중간 특성들(`encoder_features`) 가운데 각 디코더 블록으로 전달할 특성을 선택해야합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrWfFy0aH4lL"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, model_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = None # Hint. nn.ModuleList\n",
        "\n",
        "        self.out = nn.Conv2d(model_channels, out_channels, kernel_size=1) # 1 x 1 conv\n",
        "\n",
        "    def forward(self, x, encoder_features):\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            pass\n",
        "        x = self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bzM_U8tfQrn"
      },
      "source": [
        "## Step 3. U-Net 정의\n",
        "\n",
        "위에서 정의한 인코더와 디코더를 사용해서 U-Net을 완성하세요.\n",
        "\n",
        "<br>\n",
        "\n",
        "우선, 인코더와 디코더의 입출력 채널 수는 다음과 같습니다.\n",
        "* Encoder.  `in_channels` → `hidden_channels`\n",
        "* Decoder.  `hidden_channels * 16` → `out_channels`\n",
        "\n",
        "<br>\n",
        "\n",
        "이어서 인코더와 디코더를 연결하는 **브릿지 모듈**을 정의해줍니다.\n",
        "\n",
        "* 브릿지 모듈은 두 개의 Conv2d 레이어로 이루어집니다. 마찬가지로 모든 Conv2d 레이어의 출력은 ReLU 활성화함수를 통과하도록 설계합니다.\n",
        "* 두 개의 Conv2d 레이어 모두 (kernel_size, stride, padding) = (3, 1, 1)로 설정합니다. 따라서 Conv2d 레이어의 출력은 입력과 동일한 모양을 갖습니다.\n",
        "* 첫번째 Conv2d 레이어에서는 입출력 채널 수가 `hidden_channels * 8` → `hidden_channels * 16`으로 변하도록, 두번째 Conv2d 레이어에서는 `hidden_channels * 16` → `hidden_channels * 16`이 되도록 정의합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5qNmlzHL-d8"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = None\n",
        "        self.bridge = None # Hint. nn.Sequential\n",
        "        self.decoder = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, encoder_features = self.encoder(x)\n",
        "        x = self.bridge(x)\n",
        "        x = self.decoder(x, encoder_features)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oRXqbufKbSp"
      },
      "source": [
        "모델이 제대로 정의되었는지 확인합니다. 잘 정의되었다면 다음과 같이 출력됩니다.\n",
        "\n",
        "> input shape: torch.Size([1, 3, 256, 256])  \n",
        "output shape: torch.Size([1, 3, 256, 256])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gPd8L74kaXP"
      },
      "outputs": [],
      "source": [
        "unet = UNet(3, 16, 3)\n",
        "\n",
        "random_input = torch.randn(1, 3, 256, 256)\n",
        "random_output = unet(random_input)\n",
        "\n",
        "print('input shape:', random_input.shape)\n",
        "print('output shape:', random_output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YlUgfq3qKqs"
      },
      "source": [
        "## Step 3. 데이터 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyI_6LYjY3_n"
      },
      "source": [
        "이번 튜토리얼에서는 Caltech101 데이터셋을 사용합니다. torchvision.datasets의 Caltech101 객체를 정의하세요. 이때 다음과 같이 정의된 전처리를 `transfrom`의 값으로 전달해줘야합니다.  \n",
        "\n",
        "*  PIL Image를 텐서로 변환 (`ToTensor`)\n",
        "*  크기가 `image_size` x `image_size`가 되도록 조정 (`Resize`)\n",
        "*  이미지를 흑백, 단일 채널로로 변경 (`Grayscale`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1oljQzkQqRU"
      },
      "outputs": [],
      "source": [
        "data_root = './data'\n",
        "\n",
        "transform = None # Hint. transforms.Compose\n",
        "\n",
        "dataset = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L438k0WZ6jH"
      },
      "source": [
        "데이터셋을 3개의 subset: train, validation, test (8:1:1)로 구분해줍니다. 이때 torch.utils.data 패키지의 `random_split` 함수를 사용하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bZ_wZdmSMzp"
      },
      "outputs": [],
      "source": [
        "print(len(dataset)) # 8677\n",
        "\n",
        "train_dataset, validation_dataset, test_dataset = None, None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aom18lktaQM6"
      },
      "source": [
        "각 데이터셋에 대한 데이터 로더를 정의하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA6TW965SrkZ"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiCUIpUPaUoA"
      },
      "source": [
        "샘플 배치의 모양을 확인합니다. 올바르게 정의되었다면 아래와 같이 출력됩니다.\n",
        "\n",
        "> torch.Size([64, 1, 128, 128])   \n",
        "torch.Size([64])\n",
        "\n",
        "또한 아래 코드는 일부 샘플 이미지들을 보여줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpK6Mxt8TYSl"
      },
      "outputs": [],
      "source": [
        "sample_data, sample_label = next(iter(train_loader))\n",
        "\n",
        "print(sample_data.shape, sample_label.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "print(\"Sample Labels\")\n",
        "print(sample_label)\n",
        "plt.imshow(np.transpose(vutils.make_grid(sample_data[:64], nrow=8, padding=2, normalize=True), (1, 2, 0)))\n",
        "plt.title(f'Sample Images')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lPdS6MVYNqO"
      },
      "source": [
        "## Step 4. 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvhGdQMCaojz"
      },
      "source": [
        "학습에 사용할 장치(GPU 또는 CPU)를 설정합니다. `device`의 값이 GPU가 사용가능한 환경이라면 'cuda', 그렇지 않으면 'cpu'가 되도록 하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cvIMkLKWYFk"
      },
      "outputs": [],
      "source": [
        "device = None\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqxCi-sca7DX"
      },
      "source": [
        "모델과 옵티마이저를 정의하세요. 그리고 모델을 위에서 설정한 장치로 옮겨주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB4avSSoWGSr"
      },
      "outputs": [],
      "source": [
        "unet = UNet(1, 16, 1).to(device) # (in channels) = (out channels) = 1, since the images are in grayscale\n",
        "\n",
        "optimizer1 = optim.Adam(unet.parameters(), lr=learning_rate)\n",
        "criterion1 = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PGm91OObDRs"
      },
      "source": [
        "모델 학습을 시작합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V0ltbFKXB7U"
      },
      "outputs": [],
      "source": [
        "global_step = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAb4ZieaS2Gi"
      },
      "outputs": [],
      "source": [
        "unet.train()\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), desc='Epoch'):\n",
        "    for x, _ in tqdm(train_loader, desc='Iteration', leave=False):\n",
        "\n",
        "        # TODO\n",
        "\n",
        "        if global_step % 30 == 0:\n",
        "            writer1.add_scalar(\"stage1_train/loss\", loss, global_step)\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "writer1.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er9L4-xjYR1-"
      },
      "source": [
        "## Step 5. 모델 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT_-vBsDYRSs"
      },
      "source": [
        "학습이 제대로 진행되었는지 원본 이미지와 재구성한 이미지를 비교하세요. 두 개의 이미지 격자에서 동일한 위치에 있는 이미지들은 서로 대응되어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7dFTS2Ibf_H"
      },
      "outputs": [],
      "source": [
        "sample_batch, _ = next(iter(test_loader))\n",
        "\n",
        "plt.figure(figsize=(12, 18))\n",
        "\n",
        "# 실제 이미지\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(sample_batch[:64], nrow=8, padding=2, normalize=True), (1, 2, 0)))\n",
        "\n",
        "# 재구성 이미지\n",
        "unet.eval()\n",
        "with torch.no_grad():\n",
        "    recon_batch = unet(sample_batch.to(device))\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Reconstructed\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(recon_batch[:64].cpu(), nrow=8, padding=2).cpu(), (1, 2, 0)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G692mvbckII"
      },
      "source": [
        "## Step 6. 학습된 표현을 사용해서 downstream 분류 과제 수행"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage 2을 위한 하이퍼파라미터"
      ],
      "metadata": {
        "id": "gDoTIr_wzktP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "learning_rate = 1e-3"
      ],
      "metadata": {
        "id": "VqmqF7M3znki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSqUqVn3doi4"
      },
      "source": [
        "### Step 6-1. 간단한 Classification Head 정의\n",
        "\n",
        "ClassificationHead는 U-Net Encoder의 마지막 feature를 입력받아 클래스 확률을 출력하는 모듈입니다. 이 모듈은 세 개의 레이어: Conv2d, Flatten, Linear로 이루어져 있습니다.\n",
        "\n",
        "*  Conv2d는 (kernel_size, stride) = (1, 1)로 정의하여 입출력 맵의 크기는 같게 유지하되 채널 수만 바꿔줍니다. 이때, 출력 채널 수가 hidden_channels가 되어야 합니다.\n",
        "*  Linear는 클래스 확률을 출력해야하므로 출력 채널 수가 num_classes가 되도록 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIwBEFJucriY"
      },
      "outputs": [],
      "source": [
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, hidden_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.classifier = None # Hint. nn.Sequential\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkNIB8Z2dspy"
      },
      "source": [
        "### Step 6-2. 전체 Classification 모델 정의\n",
        "\n",
        "\n",
        "UNetClassifier는 U-Net Encoder와 ClassificationHead로 이루어져 있습니다. 이때, 표현을 추출하는 U-Net Encoder 부분은 더 이상 업데이트하지 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REf5OdsJdmwu"
      },
      "outputs": [],
      "source": [
        "class UNetClassifier(nn.Module):\n",
        "    def __init__(self, encoder, hidden_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = ClassificationHead(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            x, _ = self.encoder(x) # (B, 128, 8, 8), 인코더는 업데이트 되지 않기를 원함\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA9mJ3bld09I"
      },
      "source": [
        "### Step 6-3. 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c4HxmmahghX"
      },
      "outputs": [],
      "source": [
        "now = datetime.now()\n",
        "formattedDate = now.strftime(\"%H%M%S\")\n",
        "print(formattedDate)\n",
        "\n",
        "run_name = f'classifier-{formattedDate}'\n",
        "\n",
        "log_dir = os.path.join('./runs', run_name)\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "writer2 = SummaryWriter(log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzwBMEfWhwHz"
      },
      "outputs": [],
      "source": [
        "global_step2 = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델과 손실함수, 옵티마이저를 정의합니다. 손실함수로는 CrossEntropyLoss를, 옵티마이저로는 Adam을 사용합니다.   \n",
        "\n",
        "Hint. 옵티마이저가 어떤 파라미터를 업데이트해야하나요?"
      ],
      "metadata": {
        "id": "mZUzWzgqtWsJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0PsdWUjeKAH"
      },
      "outputs": [],
      "source": [
        "model = UNetClassifier(None, 16, 101).to(device) # Fill this None\n",
        "\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "optimizer2 = optim.Adam(None, lr=0.001) # Fill this None\n",
        "scheduler2 = optim.lr_scheduler.StepLR(optimizer2, step_size=5, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 학습합니다."
      ],
      "metadata": {
        "id": "t8nnpkIOtzsp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS0zkfDkd0C8"
      },
      "outputs": [],
      "source": [
        "for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
        "\n",
        "    for image, label in tqdm(train_loader, desc='Iteration', leave=False):\n",
        "\n",
        "        # TODO:\n",
        "\n",
        "        if global_step2 % 10 == 0:\n",
        "            writer2.add_scalar(\"stage2_train/loss\", loss.item(), global_step)\n",
        "\n",
        "            # 정확도 평가\n",
        "            correct = None # Hint. torch.argmax\n",
        "            total = len(label)\n",
        "            acc = correct / total * 100\n",
        "            writer2.add_scalar(\"stage2_train/acc\", acc.item(), global_step)\n",
        "\n",
        "        global_step2 += 1\n",
        "\n",
        "    scheduler2.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.6f}, Acc: {acc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHsgrsGRf-KN"
      },
      "source": [
        "### Step 6-4. 모델 평가\n",
        "\n",
        "평가 데이터셋에 대한 **정확도(맞춘 샘플 수/전체 샘플 수 * 100)**를 계산하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzyIJwH4f9hX"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for input, label in tqdm(test_loader, desc='Iteration', leave=False):\n",
        "        input = input.to(device)\n",
        "        label = label.to(device)\n",
        "        pred = model(input)\n",
        "\n",
        "        correct += None # Hint. torch.argmax\n",
        "        total += len(label)\n",
        "\n",
        "    # 정확도 계산\n",
        "    acc = correct / total * 100\n",
        "    print(f\"Test Accuracy: {acc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 코드는 평가 이미지 12장과 그에 대한 (정답, 예측) 클래스를 보여줍니다."
      ],
      "metadata": {
        "id": "MeX2ai3YvWCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplots(3, 4, figsize=(10, 8))\n",
        "\n",
        "indices = torch.randint(0, len(test_dataset), (12,))\n",
        "print(indices)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, idx in enumerate(indices):\n",
        "        img, label = test_dataset[idx]\n",
        "        img = img.to(device)\n",
        "        pred = model(img.unsqueeze(0)) # 배치 차원 추가\n",
        "\n",
        "        pred_class = torch.argmax(pred, dim=1).item() # tensor([v]) -> v\n",
        "        pred_prob = F.softmax(pred, dim=1)\n",
        "\n",
        "        plt.subplot(3, 4, i+1)\n",
        "        plt.imshow(img.squeeze().cpu(), cmap='gray') # 배치 차원 제거\n",
        "        plt.title(f\"True: {label} / Pred: {pred_class} ({pred_prob[0][pred_class].item():.2f})\")\n",
        "        plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2Il-gi2_vVWG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyN+i1UcgkDigUHNt7hSnLfg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}